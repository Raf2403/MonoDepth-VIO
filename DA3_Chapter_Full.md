# ГЛАВА 11: DEPTH ANYTHING 3 - УНИФИЦИРОВАННЫЙ ПОДХОД К ВОССТАНОВЛЕНИЮ ВИЗУАЛЬНОЙ ГЕОМЕТРИИ
---

## 1. Введение в архитектуру Depth Anything 3

**Depth Anything 3 (DA3)** — это принципиально новый подход к восстановлению трёхмерной геометрии сцены из произвольных визуальных входных данных, разработанный командой ByteDance Research и выпущенный 14 ноября 2025 года. Модель демонстрирует два ключевых архитектурных решения, которые отличают её от предыдущих поколений:

1. **Использование простого ванильного трансформера** (vanilla DINO encoder) без специализированных архитектурных модификаций;
2. **Введение унифицированного представления глубины-луча** (depth-ray representation), которое устраняет необходимость в сложном многозадачном обучении.

В отличие от **Depth Anything 2**, которая использовала различные архитектурные подходы для разных сценариев, DA3 предлагает монолитное решение, способное одновременно работать с одиночными изображениями, мультивью последовательностями и ситуациями с известными или неизвестными позами камер. Это достигается благодаря универсальной глубинно-лучевой парадигме, которая кодирует информацию о глубине и направлении луча в единообразном формате.

**Цитирование:** Lin H., Chen S., Liew J.H., et al. (2025). "Depth Anything 3: Recovering the visual space from any views." arXiv preprint arXiv:2511.10647.

---

## 2. Ключевые компоненты архитектуры

### 2.1 Кодер-базис (Encoder Backbone)

Модель использует **DinoV2** в качестве основного кодера, представляющего собой чистый Vision Transformer без специальных модификаций. Стандартная конфигурация включает:

- **Основание:** ViT-B (Vision Transformer-Base)
- **Выходные слои:** [5, 7, 9, 11]
- **Техники нормализации:** 
  - $qknorm$ (Query-Key normalization) начиная со слоя 4
  - $rope$ (Rotary Position Embeddings) начиная со слоя 4
- **Дополнительно:** Конкатенация токена класса (`cat_token: True`)

Такой выбор кодера обеспечивает высокую обобщающую способность модели без избыточной специализации, позволяя модели работать на разнообразных сценах и типах камер.

### 2.2 Голова-декодер (DualDPT Head)

Вслед за кодером идёт **DualDPT** — двойной декодер с плотной архитектурой, который преобразует признаки из кодера в предсказания глубины-луча. Архитектурные параметры:

- **Входная размерность:** 1536 (выход ViT-B)
- **Выходная размерность:** 2 (компоненты представления глубины-луча)
- **Внутренние каналы:** [96, 192, 384, 768]
- **Внутренние признаки:** 128

DualDPT реализует асимметричный дизайн декодера, позволяя эффективно обрабатывать многомасштабные признаки от кодера и генерировать плотные предсказания на полном разрешении входного изображения.

### 2.3 Представление глубины-луча (Depth-Ray Representation)

Центральное нововведение DA3 — это унифицированное представление, которое кодирует как метрическую глубину $d$, так и направление луча в пространстве камеры $\mathbf{r}$:

$$\text{Depth-Ray} = (d, \mathbf{r}) \in \mathbb{R}^{1} \times \mathbb{R}^{3}$$

где $\mathbf{r}$ — единичный вектор, определяющий направление луча в 3D пространстве камеры. Это позволяет модели одновременно предсказывать:

- **Геометрию в абсолютном масштабе** (при наличии дополнительной информации о позе или калибровке)
- **Относительную структуру сцены** (для моноскулярных случаев без масштаба)

Такое представление элегантно решает проблему амбивалентности масштаба, которая является фундаментальной для монокулярной оценки глубины, и одновременно позволяет использовать информацию о позе камеры при её наличии.

---

## 3. Семейство моделей и их назначение

### 3.1 Серия DA3 Main Series

Включает четыре масштаба: **DA3-Giant**, **DA3-Large**, **DA3-Base** и **DA3-Small**. Все модели обучены унифицированной парадигме глубины-луча и могут выполнять множество задач путём варьирования конфигурации входных данных.

**Параметры моделей:**

| Модель    | Параметры | Применение                                |
| --------- | --------- | ----------------------------------------- |
| DA3-Giant | 1.15B     | Highest quality multi-view reconstruction |
| DA3-Large | 0.35B     | Production multi-view & monocular         |
| DA3-Base  | 0.12B     | Mobile & embedded systems                 |
| DA3-Small | 0.08B     | **Recommended for Raspberry Pi 4**        |

**Поддерживаемые задачи:**

1. **Моноскулярная оценка глубины** — предсказание карты глубины с одиночного RGB-изображения
2. **Мультивью оценка глубины** — генерация согласованных карт глубины из нескольких изображений для высокачественного слияния
3. **Оценка глубины с условием позы** — достижение превосходной пространственной согласованности глубины при условии известных параметров позы камеры
4. **Оценка позы камеры** — предсказание как внешних параметров (матрица вращения $\mathbf{R}$, вектор трансляции $\mathbf{t}$), так и внутренних параметров (матрица калибровки $\mathbf{K}$)
5. **Оценка трёхмерных гауссианов** — прямое предсказание 3D гауссов для синтеза новых видов высокой верности

### 3.2 Серия DA3 Metric Series

**DA3Metric-Large** — специализированная модель, которая специализирована для метрической оценки глубины в моноскулярных сценариях. Эта модель идеальна для приложений, требующих восстановления геометрии в реальном масштабе:

- Структурированное сканирование зданий и помещений
- Приложения робототехники, требующие абсолютного масштаба
- Картирование больших открытых пространств

**Ключевая особенность:** Специальное обучение на датасетах, содержащих метрическую информацию глубины, что позволяет модели напрямую предсказывать абсолютные глубины в единицах измерения (метры).

### 3.3 Серия DA3 Monocular Series

**DA3Mono-Large** — специализированная модель для высококачественной относительной моноскулярной оценки глубины. В отличие от модифицированных моделей диспаритета (например, Depth Anything 2), DA3Mono-Large напрямую предсказывает глубину, что результирует в превосходной геометрической точности.

### 3.4 Вложенная серия (Nested Series)

**DA3Nested-Giant-Large** объединяет гигантскую модель общего назначения (DA3-Giant) с метрической моделью (DA3Metric-Large) для восстановления визуальной геометрии в абсолютном метрическом масштабе. Эта комбинация обеспечивает:

- Высокую геометрическую согласованность (DA3-Giant)
- Абсолютные размеры восстановленной сцены (DA3Metric-Large)

---

## 4. Возможности и применение

### 4.1 Оценка параметров камеры

Модель может одновременно предсказывать как внешние параметры (R, t) в формате OpenCV w2c (world-to-camera) или COLMAP, так и внутренние параметры камеры (калибровочную матрицу K):

$$\text{Extrinsics} = [\mathbf{R} | \mathbf{t}] \in \mathbb{R}^{3 \times 4}$$
$$\text{Intrinsics} = \mathbf{K} = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} \in \mathbb{R}^{3 \times 3}$$

Это особенно полезно для задач структуры из движения (SfM) и инициализации систем визуального одометра (VO).

### 4.2 Мультивью синтез и слияние

Путём обработки последовательностей изображений с различными углами зрения модель генерирует согласованные карты глубины, которые могут быть использованы для высококачественной реконструкции поверхности методами:

- **Слияния точек облака** (point cloud fusion)
- **Марширующих кубов** (marching cubes для сетки)
- **Пуассоновой реконструкции поверхности** (Poisson surface reconstruction)

### 4.3 Синтез новых видов с 3D гауссовыми сплатами

Прямое предсказание трёхмерных гауссовых сплатов позволяет DA3 обеспечивать высокоточный синтез произвольных видов сцены без явной реконструкции сетки или облака точек. Это составляет основу для:

- Высокоэффективных приложений виртуальной реальности (VR)
- Интерактивной визуализации архитектурных сцен
- Синтеза новых видов в реальном времени

---

## 5. Интеграция с Raspberry Pi 4

### 5.1 Выбор модели

Для применения на платформе **Raspberry Pi 4**, рекомендуется использование компактных моделей:

- **DA3-Small (80M параметров)** — минимум требований, приемлемое качество
- **DA3-Base (120M параметров)** — оптимальный баланс качества и производительности

Несмотря на меньшее количество параметров, эти модели сохраняют способность выполнять основные задачи восстановления геометрии благодаря унифицированной парадигме DA3.

### 5.2 Оптимизация для встроенных систем

Для оптимизации производительности на встроенных устройствах рекомендуется:

1. **Квантование весов модели:**
   - FP32 → FP16 (half precision) для сокращения использования памяти в 2 раза
   - FP16 → INT8 (post-training quantization) для дополнительного ускорения

2. **Техники дистилляции знаний:**
   - Обучение компактных моделей (DA3-Base, DA3-Small) на выходах крупных моделей (DA3-Large, DA3-Giant)
   - Сохранение качества при значительном снижении параметров

3. **Батчирование и масштабирование:**
   - Обработка изображений с пониженным разрешением (512×384 вместо 1024×768)
   - Использование single-image inference вместо multi-view для частых срабатываний

4. **Кэширование активаций:**
   - Кэширование выходов слоёв кодера для повторяющихся последовательностей видео
   - Асинхронная обработка глубины с отделением от основного цикла управления

### 5.3 Примерная производительность на Pi4

**Ожидаемые характеристики (предварительная оценка):**

- **DA3-Small FP16:** ~8–12 FPS при разрешении 512×384
- **DA3-Base FP16:** ~4–6 FPS при разрешении 640×480
- **Потребление памяти:** 1.5–2.5 GB RAM (в пределах 4GB Pi4)
- **Потребление GPU:** До 70% расчётной способности

**Рекомендация:** Использовать асинхронную обработку глубины в отдельном потоке с частотой 10 Hz, синхронизируя основной цикл управления приводом на 30 Hz с последней доступной картой глубины.

---

## 6. Интеграция с дифференциальным приводом

### 6.1 Оценка позы для локализации

Предсказываемые параметры позы камеры $(R, t)$ и карты глубины из DA3 могут быть напрямую использованы для навигации и управления дифференциальным приводом робота.

**Внешние параметры камеры:**

Матрица вращения $\mathbf{R} \in SO(3)$ и вектор трансляции $\mathbf{t} \in \mathbb{R}^3$ восстанавливают относительное положение и ориентацию робота между кадрами:

$$\mathbf{p}_{cam} = \mathbf{R} \mathbf{p}_{world} + \mathbf{t}$$

Эта информация критична для локализации робота в трёхмерном пространстве и может быть использована как вход для систем:

- **Визуальной одометрии (VO)** — оценка движения робота между кадрами
- **Визуально-инерциального одометра (VIO)** — слияние информации с IMU датчиков

### 6.2 Препятствия и навигация

Оценённые карты глубины $D(u, v)$ позволяют роботу идентифицировать препятствия в его окружении. Путём сегментации и анализа карты глубины система навигации может:

1. **Определить безопасные траектории движения** — выделить области без препятствий на карте глубины
2. **Рассчитать команды управления приводом** — определить требуемые скорости левого и правого моторов $(\omega_L, \omega_R)$ для избежания столкновений
3. **Локальное планирование** — использовать карту глубины как ограничения для алгоритмов локального планирования пути (например, Dynamic Window Approach)

**Типовая обработка карты глубины:**

$$D_{risk}(u, v) = \begin{cases} 1 & \text{if } D(u, v) < d_{threshold} \text{ (близкое препятствие)} \\ 0 & \text{otherwise} \end{cases}$$

где $d_{threshold}$ — пороговое расстояние безопасности (обычно 0.5–1.0 м).

---

## 7. Сравнение с Depth Anything 2

### 7.1 Архитектурные различия

| Характеристика | Depth Anything 2 | Depth Anything 3 |
|---|---|---|
| **Кодер** | Специализированные модули | Vanilla DINO (ViT-B) |
| **Декодер** | Адаптивные архитектуры | Unified DualDPT |
| **Представление** | Диспаритет/глубина | Depth-Ray (унифицированное) |
| **Многозадачность** | Множество loss функций | Одна unified loss |
| **Масштабируемость** | Разные модели для разных задач | Одна модель для всех |

### 7.2 Производительность

**На основе официального репозитория:**

- **Моноскулярная оценка глубины:** DA3 **значительно превосходит DA2**
  - Лучшие метрики RMSE, δ¹ (percentage of valid pixels)
  
- **Мультивью оценка глубины:** DA3 **сравнима с VGGT** (State-of-the-art)
  - Консистентность между видами
  - Точность слияния
  
- **Оценка позы камеры:** DA3 **поддерживает нативно**, DA2 требует дополнительных модулей

### 7.3 Обобщение

**Ключевое преимущество:** Все модели серии DA3 обучены исключительно на открытых академических наборах данных, что обеспечивает хорошее обобщение на новые сцены, камеры и условия без переобучения на специализированных датасетах.

---

## 8. Инструментарий и экспорт результатов

### 8.1 Веб-интерфейс (Web UI)

Интерактивный **Gradio-based интерфейс** позволяет пользователям:

- Загружать изображения или видео
- Визуализировать результаты в реальном времени
- Сравнивать выходы разных моделей
- Просматривать 3D реконструкцию в интерактивной галерее

### 8.2 Командная строка (CLI)

Мощный **Command-Line Interface** поддерживает:

- **Пакетную обработку** изображений и видео
- **Автоматический режим** (auto) для стандартных сценариев
- **Видеообработку** с настройкой FPS и экспорта

**Примеры использования:**

```bash
# Обработка папки с изображениями
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir results/ \
    --use-backend

# Обработка видео
da3 video robot_video.mp4 \
    --fps 15 \
    --export-dir video_results/ \
    --export-format glb-feat_vis
```

### 8.3 Форматы экспорта

Результаты могут быть экспортированы в множество форматов:

| Формат | Описание | Применение |
|--------|---------|-----------|
| **GLB** (glTF Binary) | Стандартный 3D формат | Визуализация, импорт в другие ПО |
| **NPZ** | NumPy compressed arrays | Обработка в Python, анализ данных |
| **PLY** | Polygon File Format | Облака точек, сетки |
| **3DGS video** | Гауссовы сплаты | Синтез новых видов, VR/AR |
| **PNG/EXR** | Depth images | Обработка изображений, анализ |

---

## 9. Практическое применение для робототехнического проекта

### 9.1 Архитектурный стек системы

```
┌─────────────────────────────────────────┐
│        Робот с RealSense D435i         │
└──────────────────┬──────────────────────┘
                   │
        ┌──────────▼──────────┐
        │   Raspberry Pi 4    │
        │   (Ubuntu 22.04)    │
        └──────────┬──────────┘
          ┌────────┴────────┐
          │                 │
    ┌─────▼─────┐    ┌─────▼──────┐
    │  ROS2 Node│    │  DA3-Base  │
    │  (30 Hz)  │    │  (10 Hz)   │
    └─────┬─────┘    └─────┬──────┘
          │                │
    ┌─────▼────────────────▼────┐
    │   Fusion & Control Logic   │
    │  (Pose + Depth Integration)│
    └─────┬──────────────────────┘
          │
    ┌─────▼────────────────┐
    │  Differential Drive  │
    │   Motor Controller   │
    └──────────────────────┘
```

### 9.2 Типовой цикл обработки

1. **Захват изображений (30 FPS):**
   - RGB от RealSense (1920×1080)
   - Depth от RealSense (1280×720)
   - IMU данные (200 Hz)

2. **Предварительная обработка (асинхронно):**
   - Масштабирование RGB к 512×384
   - Нормализация и подготовка для DA3

3. **Инференция DA3 (10 FPS):**
   - Предсказание карты глубины
   - Оценка параметров позы
   - Получение матриц R, t, K

4. **Слияние и интеграция:**
   - Комбинирование метрической глубины RealSense с относительной от DA3
   - Обновление состояния робота (pose, velocity)
   - Анализ препятствий

5. **Управление приводом (30 Hz):**
   - Расчёт требуемых скоростей моторов
   - Отправка команд на контроллер привода
   - Логирование состояния

### 9.3 Интеграция с ROS2

```python
# Пример узла ROS2 для обработки DA3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge
import torch
from depth_anything_3.api import DepthAnything3

class DA3Node(Node):
    def __init__(self):
        super().__init__('da3_node')
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = DepthAnything3.from_pretrained("depth-anything/DA3-BASE")
        self.model = self.model.to(device=self.device)
        
        self.image_sub = self.create_subscription(Image, '/camera/rgb/image_raw', 
                                                   self.image_callback, 10)
        self.depth_pub = self.create_publisher(Image, '/da3/depth', 10)
        self.pose_pub = self.create_publisher(PoseStamped, '/da3/camera_pose', 10)
        
        self.bridge = CvBridge()
    
    def image_callback(self, msg):
        # Преобразование ROS сообщения в OpenCV
        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')
        
        # Инференция DA3
        prediction = self.model.inference([frame])
        
        # Публикация результатов
        depth_msg = self.bridge.cv2_to_imgmsg(prediction.depth[0].astype('float32'), 
                                              encoding="passthrough")
        self.depth_pub.publish(depth_msg)
        
        # Публикация позы камеры
        pose_msg = PoseStamped()
        pose_msg.pose.position.x = prediction.extrinsics[0][0, 3]
        pose_msg.pose.position.y = prediction.extrinsics[0][1, 3]
        pose_msg.pose.position.z = prediction.extrinsics[0][2, 3]
        self.pose_pub.publish(pose_msg)

if __name__ == '__main__':
    rclpy.init()
    node = DA3Node()
    rclpy.spin(node)
```

---

## 10. Выводы и рекомендации

### 10.1 Ключевые вывод

1. **Архитектурная простота:** Depth Anything 3 доказывает, что ванильный трансформер без специализированных модификаций достаточен для решения сложных задач восстановления геометрии.

2. **Универсальность:** Унифицированное представление глубины-луча позволяет одной модели решать множество задач (monocular, multi-view, pose estimation), что значительно упрощает deployment.

3. **Практическая реализуемость:** Компактные модели (DA3-Small, DA3-Base) вполне применимы на встроенных системах типа Raspberry Pi 4 с приемлемой производительностью (10 Hz).

4. **Улучшение качества:** Значительное превосходство над Depth Anything 2 на моноскулярных задачах указывает на улучшение обобщающей способности и надёжности предсказаний.

### 10.2 Рекомендации для вашего проекта

1. **Выбор модели:** Используйте **DA3-Base** для оптимального баланса качества и скорости на Raspberry Pi 4

2. **Асинхронная обработка:** Запускайте DA3 в отдельном потоке с частотой ~10 Hz, основной цикл управления — 30 Hz

3. **Гибридный подход:** Комбинируйте метрическую глубину RealSense (надёжна на близких расстояниях) с относительной глубиной от DA3 (хороша на дальних)

4. **Интеграция с ROS2:** Используйте примеры кода для создания DA3Node, интегрирующегося со своей системой управления

5. **Экспортирование результатов:** Используйте NPZ формат для сохранения результатов на SD карту для последующего анализа

### 10.3 Ограничения и будущая работа

**Текущие ограничения:**

- Отсутствуют официальные бенчмарки для Raspberry Pi 4
- Требуется уточнение real-time characteristics в реальных условиях
- Влияние условий освещения (ночь, прямой солнечный свет) требует тестирования

**Направления будущей работы:**

- Файн-тюнинг DA3-Base на датасет вашего робота
- Интеграция с системой localisation & mapping (SLAM)
- Оптимизация through model compression techniques (knowledge distillation, pruning)

---

## Справочные материалы

**Основные источники:**

1. Official GitHub Repository: https://github.com/ByteDance-Seed/Depth-Anything-3
2. ArXiv Paper: arXiv:2511.10647
3. DINO Reference: Caron et al., "Emerging Properties in Self-Supervised Vision Transformers" (ICCV 2021)
4. DinoV2: Oquab et al., "DINOv2: Learning Robust Visual Features without Supervision" (2023)

**Модели:**

- DA3-Small (80M params): https://huggingface.co/depth-anything/DA3-SMALL
- DA3-Base (120M params): https://huggingface.co/depth-anything/DA3-BASE  
- DA3-Large (350M params): https://huggingface.co/depth-anything/DA3-LARGE

**Установка:**

```bash
pip install torch>=2 torchvision
pip install -e ".[all]"  # All features including Gaussians and web app
```

---


